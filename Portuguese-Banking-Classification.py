# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oVfRGhwMBtmCW57DrNmkqj4rXbaIHCUq

# Project
*   Name: B Bharath
*   Data Science
*   Email Id: bboline@gitam.in

# **Bank Client Data**
```
1 - age (numeric)
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
5 - default: has credit in default? (categorical: 'no','yes','unknown')
6 - housing: has housing loan? (categorical: 'no','yes','unknown')
7 - loan: has personal loan? (categorical: 'no','yes','unknown')

```

# **Related with the last contact of the current campaign**
```
8 - contact: contact communication type (categorical: 'cellular','telephone')
9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
11 - duration: last contact duration, in seconds (numeric).
```
Important note: This attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

# **Other Attributes**
```
12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14 - previous: number of contacts performed before this campaign and for this client (numeric)
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
```

# **Social and Economic Context Attributes**
```
16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
17 - cons.price.idx: consumer price index - monthly indicator (numeric)
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
20 - nr.employed: number of employees - quarterly indicator (numeric)
21- Has the client subscribed to a term deposit? (binary: 'yes','no')
```

# Code & Execution

1)Import the necessary libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report

"""2)Load the data as a data frame using pandas"""

# Load dataset
bank = pd.read_csv("bank-full.csv")

"""3) Shape of the data"""

# Display dataset information
print("Dataset Shape:", bank.shape)
print("Column Names:", bank.columns.tolist())
print("First 5 Rows:\n", bank.head())

"""4)Data type of each attribute"""

bank.dtypes

# Rename target column
bank.rename(columns={'y': 'Target'}, inplace=True)

"""5)Checking the presence of missing values"""

# Data Cleaning
print("Checking for missing values:\n", bank.isnull().sum())
bank.drop_duplicates(inplace=True)
print("Dataset Shape after removing duplicates:", bank.shape)

"""6)5 point summary of numerical attributes"""

# Five-number summary
print("Five-number summary:\n", bank.describe().T)

"""Checking Imbalance"""

# Check class imbalance
print("Class distribution:\n", bank['y'].value_counts())

"""**Given data set is highly imbalanced, i.e. number of data belonging to 'no' category is way higher than 'yes' category.**"""

bank['y'].value_counts(normalize=True)

"""The response rate is only 11.6%. Hence the Y(Target) variable has a high class imbalance. Hence accuracy will not be a reliable model performance measure.

FN is very critical for this business case because a false negative is a customer who will potentially subscribe for a loan but who has been classified as 'will not subscribe'. Hence the most relevant model performance measure is recall. **bold text**
"""

# Encode categorical variables
categorical_cols = bank.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if col != 'Target':
        bank[col] = LabelEncoder().fit_transform(bank[col])

# Convert target variable to binary
bank['Target'] = bank['Target'].map({'yes': 1, 'no': 0})

# Exploratory Data Analysis (EDA)
plt.figure(figsize=(10,6))
sns.countplot(x='Target', data=bank)
plt.title("Distribution of Target Variable")
plt.show()

# Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(bank.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Matrix")
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(x='job', data=bank, order=bank['job'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Job Type Distribution")
plt.show()

# Additional Visualizations
plt.figure(figsize=(10,5))
sns.histplot(bank['age'], bins=20, kde=True)
plt.title("Age Distribution")
plt.show()

plt.figure(figsize=(10,5))
sns.boxplot(x='education', y='age', data=bank)
plt.xticks(rotation=90)
plt.title("Age vs Education")
plt.show()

plt.figure(figsize=(10,5))
sns.violinplot(x='marital', y='balance', data=bank)
plt.title("Balance Distribution by Marital Status")
plt.show()

plt.figure(figsize=(10,5))
plt.pie(bank['marital'].value_counts(), labels=bank['marital'].value_counts().index, autopct='%1.1f%%')
plt.title("Marital Status Distribution")
plt.show()

plt.figure(figsize=(10,5))
sns.stripplot(x='job', y='age', data=bank, jitter=True)
plt.xticks(rotation=90)
plt.title("Age vs Job Type")
plt.show()

sns.pairplot(bank, hue="Target")
plt.show()

# Feature Selection & Preprocessing
X = bank.drop(columns=['Target'])
y = bank['Target']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splitting dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Model Training
models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42),
    "Ada Boosting": AdaBoostClassifier(n_estimators=100, random_state=42),
    "Bagging Classifier": BaggingClassifier(n_estimators=100, random_state=42),
    "Support Vector Machine": SVC(kernel='rbf', probability=True, random_state=42),
    "Naive Bayes": GaussianNB(),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5)
}

results = {}
k_fold = KFold(n_splits=10, shuffle=True, random_state=42)
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_proba) if y_proba is not None else "N/A",
        "Cross Validation Score": cross_val_score(model, X_train, y_train, cv=k_fold, scoring='accuracy').mean()
    }

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_proba) if y_proba is not None else "N/A",
        "Cross Validation Score": cross_val_score(model, X_train, y_train, cv=k_fold, scoring='accuracy').mean()
    }

    # Confusion Matrix Visualization
    plt.figure(figsize=(6,4))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # Classification Report
    print(f"Classification Report for {name}:\n", classification_report(y_test, y_pred))

for name, model in models.items():
    y_pred = model.predict(X_test)

    # Classification Report
    print(f"Classification Report for {name}:\n", classification_report(y_test, y_pred))
    print("="*80)  # Separator for better readability

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

"""**Best Model Based on Performance Metrics, To determine the best model, we need to focus on key metrics:**

*   Accuracy – Higher is better.
*   Precision – Important if false positives are costly.
*   F1 Score – Balances precision & recall (good for imbalanced datasets).
*  ROC AUC – Measures how well the model distinguishes between classes.
*   Cross-Validation Score – Ensures the model generalizes well to unseen data.

**RandomForest is the Best Model**
"""